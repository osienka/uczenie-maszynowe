{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "Kopia notatnika 05M_Bayes_spam.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNDQCDSFVdQ4"
      },
      "source": [
        "# **SPAM vs. naiwny klasyfikator Bayesa**\n",
        "Autor: Anna Dawid\n",
        "\n",
        "Rozwiązanie: Zofia Hendrysiak"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "seqEdVR1VdQ7"
      },
      "source": [
        "## Wprowadzenie\n",
        "Nigeryjski książę wciąż zarabia na użytkownikach elektronicznych skrzynek pocztowych ponad 700 tys. dolarów rocznie ([źródło](https://www.cnbc.com/2019/04/18/nigerian-prince-scams-still-rake-in-over-700000-dollars-a-year.html))! Jak to możliwe?\n",
        "\n",
        "Pierwsza przyczyna jest natury psychologicznej. Ofiary są poddawane \"perfekcyjnej burzy pokuszeń\", jak ujął to psycholog w wywiadzie, do którego linka dałam Wam powyżej. Spammerzy łączą granie na ludzkiej chciwości, ale także na pragnieniu bycia bohaterem. W końcu kto nie chciałby zarobić na byciu wspaniałomyślnym i szczodrym? W tej kwestii możemy pracować wyłącznie nad sobą.\n",
        "\n",
        "Możemy za to pracować nad filtrami antyspamowymi. Użyjemy techniki, która nazywa się \"worek ze słowami\" (bag of words) w połączeniu z naiwnym klasyfikatorem Bayesa. Choć to prosty klasyfikator, z powodzeniem jest używany współcześnie (np. [SpamAssassin](https://cwiki.apache.org/confluence/display/spamassassin/BayesInSpamAssassin)).\n",
        "\n",
        "Notebook oparty na tutorialach:\n",
        "*   https://towardsdatascience.com/spam-classifier-in-python-from-scratch-27a98ddd8e73\n",
        "*   https://towardsdatascience.com/spam-filtering-using-naive-bayes-98a341224038\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cWBDlNPHVdQ9"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ZJI7KHjVdQ7"
      },
      "source": [
        "## Import danych treningowych\n",
        "https://www.kaggle.com/uciml/sms-spam-collection-dataset\n",
        "\n",
        "To dane przygotowane przez Almeida et al. na podstawie forum brytyjskiego, gdzie użytkownicy skarżą się na spamowe SMSy. Każdy wiersz składa się z kolumny opisującej czy wiadomość jest spamem, czy nie ('spam' czy 'ham'), a druga zawiera treść wiadomości."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x110k8pa-KD8"
      },
      "source": [
        "Proszę pobrać repozytorium ```uczenie-maszynowe-2021-22``` z serwisu github i uaktualnić ścieżkę do danych."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLHnLiSv-s6v"
      },
      "source": [
        "#!git clone https://github.com/akalinow/uczenie-maszynowe-2021-22\n",
        "folder = './dane/'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHEGrXdx97uU"
      },
      "source": [
        "df = pd.read_csv(...)\n",
        "print(df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "054a8tEtDWw2"
      },
      "source": [
        "Dane zawierają zbyteczne kolumny. Proszę:\n",
        "* usunać kolumny zawierająca wartości \"NaN\"\n",
        "* zmienić nazwy kolumn \"v1\" i \"v2\" na \"label\", \"text\"\n",
        "\n",
        "**Wskazówka**: proszę użyć metod ```DataFrame.drop()``` oraz ```DataFrame.rename()```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_e503qPbB-xa"
      },
      "source": [
        "df = df....\n",
        "df = df....\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xr86NzYAVdRF"
      },
      "source": [
        "Proszę wypisać na ekran treść maila o indeksie **57**\n",
        "\n",
        "**Wskazówka**: Indeksy obiektu DataFreame uzyskujemy przez pole ```DataFrame.index```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kBsb284aVdRG"
      },
      "source": [
        "index = 57\n",
        "print(df[...])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdn7hSzUVdRJ"
      },
      "source": [
        "Treść wiadomości:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wLflQN1VdRP"
      },
      "source": [
        "Proszę wypisać na ekran liczebność danych, czyli liczbę maili."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "575SnOW9VdRP"
      },
      "source": [
        "print(\"Data contains {} emails.\".format(...))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFpUrBiRgFiV"
      },
      "source": [
        "## Analiza częstości występowania słów w obu klasach za pomocą biblioteki WordCloud\n",
        "\n",
        "To biblioteka pozwalająca generować śliczne obrazki, na których wielkość słów odpowiada częstości jego występowania w danym zbiorze."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lpfU-WXKg5CY"
      },
      "source": [
        "!pip3 install wordcloud\n",
        "import wordcloud"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L28LyPkEivX2"
      },
      "source": [
        "from wordcloud import WordCloud\n",
        "\n",
        "# najpierw słowa ze spamu\n",
        "spam_words = \" \".join(list(df [df['label']=='spam']['text'] ))\n",
        "spam_plot = WordCloud(width = 512, height = 512).generate(spam_words)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(spam_plot);\n",
        "\n",
        "# teraz słowa z normalnych wiadomosci\n",
        "ham_words = \" \".join(list(df [df['label']=='ham']['text'] ))\n",
        "ham_plot = WordCloud(width = 512, height = 512).generate(ham_words)\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "plt.imshow(ham_plot);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxelOYTFVdRR"
      },
      "source": [
        "Dane w tej chwili są w postaci ciągów słów. Zamienimy je na postać numeryczną używająć algorymtu ```CountVectorizer```.\n",
        "Na początek zróbmy to dla prostego tekstu by zrozumieć jak działa ten algorytm. Proszę:\n",
        "* zaimportować bibliotekę zawierającą algorytm:\n",
        "\n",
        "```\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "```\n",
        "* używając metody ```CountVectorizer.fit(...)``` przeprowadzić trening algorytmu na zdaniach \n",
        "\n",
        "```\n",
        "[\"Ala ma kota.\", \"Kot? Kot ma wszy.]\n",
        "```\n",
        "* wypisać na ekran słownik utworzony przez algorytm ```CountVectorizer```: ```CountVectorizer.vocabulary_```\n",
        "* wypisać na ekran listę znalezionych słów: ```CountVectorizer.get_feature_names()```\n",
        "* przepowadzić transformację zdania do postaci numerycznej: ```CountVectorizer.transform(...)```\n",
        "* przeprowadzić transformację odwrotną:```vectorizer.inverse_transform(...)```\n",
        "* wypisać na ekran wszystkie reprezentacje zdania"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qBpZWlat2ian"
      },
      "source": [
        "from ...\n",
        "text = ...\n",
        "vectorizer = CountVectorizer()\n",
        "vectorizer.fit(text)\n",
        "print(\"Vocabulary:\",...)\n",
        "print(\"Lista słów:\",...)\n",
        "text_transformed = vectorizer....\n",
        "print(\"Original text:\",text)\n",
        "print(\"Transformed text:\",text_transformed)\n",
        "print(\"Transformed text after decoding\",...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U1bwNO7-2ian"
      },
      "source": [
        "Proszę:\n",
        "\n",
        "* przeprowadzić procedurę treningu i transformacji dla danych z e-maili.\n",
        "* proszę wypisać na ekran postać oryginalną i po transformacji maila o indeksie **57**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvM-XCWXVdRV"
      },
      "source": [
        "vectorizer = ...\n",
        "vectorizer....\n",
        "text = ...\n",
        "text_transformed = ...\n",
        "\n",
        "print(...)\n",
        "print(...)\n",
        "print(...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zHsTCZS6VdRg"
      },
      "source": [
        "## Trening klasyfikatora\n",
        "\n",
        "Proszę:\n",
        "* podzielić dane na część uczącą i treningową w stosunku **7:3**\n",
        "* wytrenować klasyfikator mail korzysjając z naiwnego algorytmu Bayesa opartego o rozkład wielomianowy: ```MultinomialNB```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FyJmhv-VVdRk"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(...)\n",
        "\n",
        "# zaimportuj odpowiednią bibliotekę\n",
        "from sklearn.naive_bayes import ...\n",
        "# stwórz obiekt klasyfikatora\n",
        "model = ...\n",
        "# naucz klasyfikator na zbiorze uczącym\n",
        "model...."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kWchjUPVdRn"
      },
      "source": [
        "## Ocena jakości\n",
        "\n",
        "Korzystając z funkcji napisanych na poprzednich ćwiczeniach:\n",
        "\n",
        "* wykonać predykcję na danych testowych\n",
        "* wypisać na ekran wartości metryk o raz macierz pomyłek"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s5jBha0P3iNd"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ktsCYepybxqu"
      },
      "source": [
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "def printScores(model, X, Y):\n",
        "    ...\n",
        "\n",
        "\n",
        "printScores(model, X_test, Y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXqzqM0cVdRs"
      },
      "source": [
        "## Analiza modelu\n",
        "\n",
        "Sprawdźmy, czego właściwie maszyna się nauczyła. Analizując współczynniki modelu proszę wskazać słowa które są istotne dkla klasyfikacji.\n",
        "\n",
        "Proszę:\n",
        "* wypisać na ekran współczynniki przypisane do kolejnych słów: ```MultinomialNB.coef_[0]```\n",
        "* stworzyć listę zawierającą indeksy posortowanych współczynników: ```np.argsort(...)```\n",
        "* wypisać na ekran po 10 słów o największych i najmniejszych wartościach współczynników.\n",
        "\n",
        "**Wskazówka** by listę słów moć adresować listą indeksów, listę słów trzeba zamienić na macierz numpy."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br2t5MLZVdRt"
      },
      "source": [
        "# np.argsort zwraca indeksy w oryginalnej tablicy, które odpowiadają posortowanej tablicy, np.:\n",
        "# np.argsort([3,1,2]) ---> [1,2,0]\n",
        "# .coef_[0] zwraca nam tablicę współczynników -- po jednym dla każdej cechy. \n",
        "# Wkład do decyzji klasyfikatora jest proporcjonalny do wartości tych współczynników -- większy współczynnik = ważniejsza cecha\n",
        "feature_names = vectorizer.get_feature_names()\n",
        "feature_names = np.array(feature_names)\n",
        "coeff = model.coef_[0]\n",
        "top10 = np.argsort(coeff)[-10:]\n",
        "bottom10 = np.argsort(coeff)[:10]\n",
        "\n",
        "print(\"Słowa, które z największą pewnością wskazują maszynie, że wiadomość to spam:\")\n",
        "print(feature_names[top10])\n",
        "\n",
        "print(\"Słowa najmniej istotne przy klasyfikacji:\")\n",
        "print(feature_names[bottom10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJv9ICOnkEqB"
      },
      "source": [
        "## Działanie modelu na nieznanych słowach.\n",
        "\n",
        "Model jedynie rozpoznaje poszczególne słowa, be żadnej analizy językowej. Dodamy pewien element analizy językowej.\n",
        "Proszę:\n",
        "\n",
        "* wybrać dowolne, bezsensowne słowo które nie występuje w słowniku i sprawdzić, że tak jest\n",
        "* dokończyć zdanie : ``Life is`` tym słowem\n",
        "* wypisać postać ptrzetransformowaną, oraz jej transformację odwrotną\n",
        "* sprawdzić jak na nie zareaguje model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4G9PKlmFrCXP"
      },
      "source": [
        "word = \"...\n",
        "index = np.where(feature_names == word) \n",
        "print(\"Indeks słowa {} to: {}\".format(word, index))\n",
        "\n",
        "message = ...\n",
        "message_transformed = ...\n",
        "\n",
        "print(\"Original text:\",message)\n",
        "print(...)\n",
        "print(...)\n",
        "           \n",
        "result = ...\n",
        "print(\"Model result for senstence: \\n {} is {}\".format(message,result))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pCgizjFcmQOr"
      },
      "source": [
        "### Stemming (nawet nie będę próbować tego tłumaczyć na polski, to [bogate](https://pl.bab.la/slownik/angielski-polski/stemming) znaczeniowo słowo)\n",
        "\n",
        "Polega na ujednoliceniu słów o tym samym rdzeniu znaczeniowym (o czym maszyna, oczywiście, nie ma szans wiedzieć). Np. dzięki stemmingowi słowa \"go\", \"going\" i \"goes\" są przyporządkowane tylko jednemu słowu \"go\". Można np. użyć gotowego algorytmu stemmingowego o nazwie [Porter Stemmer](https://tartarus.org/martin/PorterStemmer/).\n",
        "\n",
        "Proszę:\n",
        "\n",
        "* wypisać na ekran indeksy słów ``going`` oraz ``go``"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oScLywVekLm4"
      },
      "source": [
        "id1 = np.where(feature_names == 'going')\n",
        "id2 = ...\n",
        "print(\"Indices for words going and go are: \"...)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3tQCtFu8JEU"
      },
      "source": [
        "!pip3 install nltk\n",
        "import nltk\n",
        "import ssl\n",
        "# chcemy pobrac wytrenowany dla języka angielskeigo tokenizer Punkt\n",
        "# poniższe linijki mają pomóc w przypadku problemów z ssl\n",
        "try:\n",
        "    _create_unverified_https_context = ssl._create_unverified_context\n",
        "except AttributeError:\n",
        "    pass\n",
        "else:\n",
        "    ssl._create_default_https_context = _create_unverified_https_context\n",
        "# pobieramy tokenizer\n",
        "nltk.download('punkt')\n",
        "# ładujemy przydatne funkcje\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mb6zYWc32iaq"
      },
      "source": [
        "# przykładowy tekst\n",
        "message = 'Applying classical methods of machine learning to the study of quantum systems (sometimes called quantum machine learning) is the focus of an emergent area of physics research'\n",
        "# tokenizujemy czyli dzielimy tekst na słowa\n",
        "words = word_tokenize(message)\n",
        "\n",
        "print(\"Message:\\n\",message)\n",
        "print(\"tokens:\\n\",words)\n",
        "\n",
        "# stwórz obiekt typu PorterStemmer\n",
        "stemmer = PorterStemmer()\n",
        "# użyj nowo stworzonego obiektu, żeby 'dostać temat' każdego ze słów\n",
        "words = [stemmer.stem(word) for word in words]\n",
        "print(\"words stems:\\n\",words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8QM_TQv2iar"
      },
      "source": [
        "Użycie klasy ```PorterStemmer()``` jako etapu pośdrednie między surowym tekstem, and ```CountVectorizer``` wymaga zdefiniowania obiektu ```analyzer```:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7yTSNBhL2iar"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "analyzer = CountVectorizer().build_analyzer()\n",
        "\n",
        "def stemmed_words(doc):\n",
        "    return (stemmer.stem(w) for w in analyzer(doc))\n",
        "\n",
        "vectorizer = CountVectorizer(analyzer=stemmed_words)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRkUybiA9oit"
      },
      "source": [
        "Powtórzmy trening i testowanie naszego klasyfikatora na danych poddanych stemmingowi. Proszę:\n",
        "\n",
        "* stworzyć zmienną zawierająca kolumnę treścni maili\n",
        "* poddać ją traqnsformacji za użyciem klasy ```PorterStemmer```\n",
        "* podzielić dane na części uczącą i testową\n",
        "* następnie wytrenować i poddać transformacji za pomocą ```CountVectorizer```\n",
        "* następnie wytrenować model i wypisać dla niego wartości metryk\n",
        "\n",
        "**Wskazówka** do transformacji tekstu użyć obiektu ```vectorizer``` zdefiniowanego w komórce powyżej."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haRIqzwm2iar"
      },
      "source": [
        "### Wracamy do pracy nad zbiorem mejli"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "asGIjB4J-Jrq"
      },
      "source": [
        "#przeprowadź serię transformacji\n",
        "text = ...\n",
        "text_stemmed = ...\n",
        "text_transformed = ...\n",
        "\n",
        "#przeprowadź podział na dane uczące i treningowe\n",
        "X_train, X_test, Y_train, Y_test = ...\n",
        "\n",
        "# stwórz obiekt klasyfikatora\n",
        "model = ...\n",
        "\n",
        "# naucz klasyfikator na zbiorze uczącym\n",
        "...\n",
        "\n",
        "# wypisz wartości metryk\n",
        "..."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WxLEc7CASHi"
      },
      "source": [
        "## Gdybyście byli spammerami... Co moglibyście zrobić, znając tę technikę antyspamową?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hOLZbs_5AY7Z"
      },
      "source": [
        "### Stosowanie znaków specjalnych zamiast liter"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qccAaFGY_C8Y"
      },
      "source": [
        "our_message = vectorizer.transform(['call for free'])\n",
        "print(model.predict(our_message))\n",
        "# podmieniamy literkę a na małpę @\n",
        "our_tricky_message = vectorizer.transform([...])\n",
        "print(model.predict(our_tricky_message))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbYzjoYFBDs-"
      },
      "source": [
        "Jakieś inne pomysły? :)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NIAyQbAtBJbI"
      },
      "source": [
        "our_tricky_message = vectorizer.transform(['Call for free sex otherwise you miss a very important meeting'])\n",
        "print(clf.predict(our_tricky_message))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q76f2JkF2ias"
      },
      "source": [
        "#### Dlaczego powyższa wiadomość została sklasyfikowana jako pożądana, chociaż jest ewidentnym przykładem spamu?"
      ]
    }
  ]
}